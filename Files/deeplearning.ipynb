{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b687eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5846db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep learning inspired by how the human brain learns \n",
    "#Neural networks are a set of algorithms, modeled loosely after the human brain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25a70f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pytorch Sensor # Similar to array or matrix \n",
    "#Tensors are the building blocks of deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88d03df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "my_list = [[1,2,3],[4,5,6]]\n",
    "my_tensor = torch.tensor(my_list)\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8757ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.int64\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "my_list = [[1,2,3],[4,5,6]]\n",
    "my_tensor = torch.tensor(my_list)\n",
    "print(my_tensor.shape)\n",
    "print(my_tensor.dtype)\n",
    "print(my_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c65301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 2],\n",
      "        [6, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "a = torch.tensor([[1,1], [2,2]])\n",
    "b = torch.tensor([[2,2], [3,3]])\n",
    "result = a * b \n",
    "print(result)\n",
    "# row * column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74762b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Neural network consisto of input output and hidden layers \\n    Input layer : dataset features \\n    Ouput layer : predictions \\n    Hidden layers : They lie between input and output layers and consist of various hidden layers \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First neural network in pytorch tensors \n",
    "''' Neural network consisto of input output and hidden layers \n",
    "    Input layer : dataset features \n",
    "    Ouput layer : predictions \n",
    "    Hidden layers : They lie between input and output layers and consist of various hidden layers \n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10673e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing as nn to avovid writing as torch.nn and \n",
    "# Input neurons : features in our dataset \n",
    "# Output : number of classes we watn to rpredit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b4faa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1981, 0.3240]], grad_fn=<AddmmBackward0>)\n",
      "Parameter containing:\n",
      "tensor([[-0.0747, -0.0305,  0.3228],\n",
      "        [-0.2709,  0.2250, -0.4945]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3139, 0.1992], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Creating input_tensor with three features\n",
    "input_tensor = torch.tensor([[0.3471, 0.4547, -0.2356]])  # Note the double square brackets\n",
    "\n",
    "# Defining the linear layer as it applies a linear transformation to the incoming data\n",
    "linear_layer = nn.Linear(\n",
    "    in_features=3,\n",
    "    out_features=2\n",
    ")\n",
    "\n",
    "# Passing the input through the linear layer\n",
    "output = linear_layer(input_tensor)\n",
    "print(output)\n",
    "print(linear_layer.weight)\n",
    "print(linear_layer.bias)\n",
    "\n",
    "# Weight reflects the importance of different features \n",
    "# bias provides neuron iwth a baseline output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc4b612f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Example for linear prediction using weight and bias \\nSuppose we are having the 3 inputs having temrpeature humidity and wind and the output of 2 then the \\nhumidity will have the more significant weight to predict whether it will be cloudy or sunny \\n\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Example for linear prediction using weight and bias \n",
    "Suppose we are having the 3 inputs having temrpeature humidity and wind and the output of 2 then the \n",
    "humidity will have the more significant weight to predict whether it will be cloudy or sunny \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49683016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=8, bias=True)\n",
      "  (1): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (2): Linear(in_features=4, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the number of input features\n",
    "n_features = 10  # Replace with the actual number of features in your data\n",
    "\n",
    "# Define the number of output classes\n",
    "n_classes = 3   # Replace with the actual number of classes in your task\n",
    "\n",
    "# Creating the network with three linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(n_features, 8),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.Linear(4, n_classes)\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e1eed2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(8,4), nn.Linear(4,2))\n",
    "# First layer has 4 neurons and each neuron has 8+ 1 9 parameters and 9 *4 = 36 parameters \n",
    "total = 0 \n",
    "for parameter in model.parameters():\n",
    "    total  = total + parameter.numel()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96603465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Activity functions added non-linearty to the network \\n    Sigmode for binary classification \\n    Softmax for multi class classification \\n    Suppose we are trying to know whether we have mamal or not we have 3 input layers limbs eggs hair and then \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Activity functions added non-linearty to the network \n",
    "    Sigmode for binary classification \n",
    "    Softmax for multi class classification \n",
    "    Suppose we are trying to know whether we have mamal or not we have 3 input layers limbs eggs hair and then \n",
    "    4 linear layers there \n",
    "     ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c60f8b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9975]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sigmoid is an activation function used in binary classification (e.g., yes/no predictions).\\n\\nIt converts logits (raw model outputs) into probabilities.\\n\\nOutput range: (0, 1) (useful for probabilities).\\n\\nExample use case: Final layer in a binary classifier (e.g., spam detection).'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "input_tensor = torch.tensor([[6]])\n",
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(input_tensor)\n",
    "print(output)\n",
    "\n",
    "'''Sigmoid is an activation function used in binary classification (e.g., yes/no predictions).\n",
    "\n",
    "It converts logits (raw model outputs) into probabilities.\n",
    "\n",
    "Output range: (0, 1) (useful for probabilities).\n",
    "\n",
    "Example use case: Final layer in a binary classifier (e.g., spam detection).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36bff007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n 1.0 |        *\\n     |       *\\n     |      *\\n     |     *\\n0.5 |----*---------\\n     |   *\\n     |  *\\n     | *\\n0.0 +----------------\\n      -6  0  6            '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " 1.0 |        *\n",
    "     |       *\n",
    "     |      *\n",
    "     |     *\n",
    "0.5 |----*---------\n",
    "     |   *\n",
    "     |  *\n",
    "     | *\n",
    "0.0 +----------------\n",
    "      -6  0  6            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "510671fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1392, 0.8420, 0.0188]])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "# 1. Create a 1x3 tensor (3 raw scores/logits for 3 classes)\n",
    "input_tensor = torch.tensor([[4.3, 6.1, 2.3]])\n",
    "\n",
    "# 2. Initialize Softmax (dim=-1 applies it to the last dimension)\n",
    "probabilities = nn.Softmax(dim=-1)\n",
    "\n",
    "# 3. Apply Softmax to convert logits to probabilities\n",
    "output_tensor = probabilities(input_tensor)\n",
    "\n",
    "# 4. Print the result\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71740a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nConcept\\tKey Takeaway\\nSoftmax\\tConverts logits to probabilities for multi-class problems. Sums to 1.\\nSigmoid\\tUsed for binary classification. Outputs independent probabilities (0 to 1).\\nUse Case\\tUse Softmax for >2 classes, Sigmoid for 2 classes.\\nPyTorch\\tnn.Softmax(dim=1) for probabilities, CrossEntropyLoss for training\\n '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Concept\tKey Takeaway\n",
    "Softmax\tConverts logits to probabilities for multi-class problems. Sums to 1.\n",
    "Sigmoid\tUsed for binary classification. Outputs independent probabilities (0 to 1).\n",
    "Use Case\tUse Softmax for >2 classes, Sigmoid for 2 classes.\n",
    "PyTorch\tnn.Softmax(dim=1) for probabilities, CrossEntropyLoss for training\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29653926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5701],\n",
      "        [0.5795],\n",
      "        [0.5344],\n",
      "        [0.5959],\n",
      "        [0.5879]], grad_fn=<SigmoidBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nHere, input dimension = 6 (6 features of each animal), output dimension = 4 (4 neurons).\\nThis reduces/expands the feature information — from 6 features into 4 new learned features\\nnn.Linear(4, 1)\\nNow the output of first layer (size 4) is passed into another linear layer.\\nInput = 4, Output = 1.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward passs : input data flows through layers calculations performed at each other \n",
    "# Output is based on weights and biases \n",
    "# Used for traning and prediction \n",
    "# Possible classifications : binary classifications , multi-class classification \n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "input_tensor = torch.tensor([[0.1234, 0.5678, 0.9012, 0.3456, 0.7890, 0.2345],\n",
    "        [0.9876, 0.4321, 0.1098, 0.6789, 0.3210, 0.8765],\n",
    "        [0.5432, 0.8765, 0.2109, 0.9012, 0.4567, 0.1234],\n",
    "        [0.7654, 0.2198, 0.5432, 0.1098, 0.8901, 0.3456],\n",
    "        [0.3210, 0.6543, 0.9876, 0.2345, 0.5678, 0.8901]])\n",
    "\n",
    "# Suppose we have 5 * 6 shape where there are 5 animals and six features \n",
    "# Creating the model using sigmoid function which is used for binary classification \n",
    "\n",
    "model = nn.Sequential(\n",
    "nn.Linear(6 ,4),\n",
    "nn.Linear(4,1),\n",
    "nn.Sigmoid() # sigmoid activation function \n",
    "\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n",
    "\n",
    "''' \n",
    "Here, input dimension = 6 (6 features of each animal), output dimension = 4 (4 neurons).\n",
    "This reduces/expands the feature information — from 6 features into 4 new learned features\n",
    "nn.Linear(4, 1)\n",
    "Now the output of first layer (size 4) is passed into another linear layer.\n",
    "Input = 4, Output = 1.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e78b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhy Linear layers?:\\n\\nThey learn the relationships between features and outputs.\\n\\nNo non-linearity until activation function is added.\\n\\nWhy Sigmoid?:\\n\\nFor binary classification to predict a probability between 0 and 1.\\n\\nOtherwise (for multiclass classification), you'd use something else like Softmax. \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Why Linear layers?\n",
    "\n",
    "They learn the relationships between features and outputs.\n",
    "No non-linearity until activation function is added.\n",
    "\n",
    "Why Sigmoid?\n",
    "For binary classification to predict a probability between 0 and 1.\n",
    "Otherwise (for multiclass classification), you'd use something else like Softmax. '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60d2abc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "tensor([[0.1915, 0.3374, 0.4711],\n",
      "        [0.1678, 0.2989, 0.5333],\n",
      "        [0.1790, 0.3133, 0.5077],\n",
      "        [0.1812, 0.3154, 0.5034],\n",
      "        [0.1718, 0.3356, 0.4927]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "''' \n",
    "class 1 - mammal \n",
    "class 2 - bird \n",
    "class 3 - reptile \n",
    "'''\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "# Creating the multiclass classification model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(6, 4),    # First Linear layer: input 6 features -> output 4 features\n",
    "    nn.Linear(4, n_classes),  # Second Linear layer: 4 features -> 3 class scores\n",
    "    nn.Softmax(dim=-1)  # Softmax activation across the classes\n",
    ")\n",
    "\n",
    "\n",
    "'''1. nn.Linear(6, 4)\n",
    "Input: 6 features (example: weight, size, temperature, etc.)\n",
    "\n",
    "Output: 4 neurons (abstract features learned by the network).\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Transform the raw input into new feature representations.\n",
    "\n",
    "2. nn.Linear(4, n_classes)\n",
    "(where n_classes = 3)\n",
    "\n",
    "Input: 4 features (from the previous layer).\n",
    "\n",
    "Output: 3 numbers (one for each class: mammal, bird, reptile).\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Produce raw scores (called logits) for each class.'''\n",
    "\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff39b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6023]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Regression model regression → the output should be a continuous real number, not probabilities.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 11 input features\n",
    "input_tensor = torch.Tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]])\n",
    "\n",
    "# Implement a neural network with exactly four linear layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(11, 64),   # Layer 1: 11 → 64\n",
    "    nn.Linear(64, 32),   # Layer 2: 64 → 32\n",
    "    nn.Linear(32, 16),   # Layer 3: 32 → 16\n",
    "    nn.Linear(16, 1)     # Layer 4: 16 → 1 (regression output)\n",
    ")\n",
    "\n",
    "output = model(input_tensor)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c61a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Output: -0.33058786392211914\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Sample input: 11 features\n",
    "input_tensor = torch.tensor([[3, 4, 6, 7, 10, 12, 2, 3, 6, 8, 9]], dtype=torch.float)\n",
    "\n",
    "# Define a simple feedforward neural network with 4 layers\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(11, 64)\n",
    "        self.layer2 = nn.Linear(64, 32)\n",
    "        self.layer3 = nn.Linear(32, 16)\n",
    "        self.output = nn.Linear(16, 1)  # Final layer for regression\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Create the model and run it on the input\n",
    "model = RegressionModel()\n",
    "output = model(input_tensor)\n",
    "\n",
    "print(\"Regression Output:\", output.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc09cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A regression model in machine learning predicts real-valued outputs (e.g., house price, temperature, or stock price). Unlike classification (which predicts discrete categories), regression answers questions like:\\n\\n\"What is the price?\"\\n\\n\"What is the expected value?\"\\n\\n\"How much will it be?\"\\n\\nArchitecture:\\n\\nnn.Linear(11, 64) – first hidden layer.\\n\\nnn.Linear(64, 32) – second hidden layer.\\n\\nnn.Linear(32, 16) – third hidden layer.\\n\\nnn.Linear(16, 1) – output layer predicting a single continuous value.\\n\\nThis kind of model can be used for tasks like:\\n\\nPredicting house prices based on 11 features (size, location, etc.)\\n\\nEstimating car mileage\\n\\nForecasting sales or demand\\n\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' A regression model in machine learning predicts real-valued outputs (e.g., house price, temperature, or stock price). Unlike classification (which predicts discrete categories), regression answers questions like:\n",
    "\n",
    "\"What is the price?\"\n",
    "\n",
    "\"What is the expected value?\"\n",
    "\n",
    "\"How much will it be?\"\n",
    "\n",
    "Architecture:\n",
    "\n",
    "nn.Linear(11, 64) – first hidden layer.\n",
    "\n",
    "nn.Linear(64, 32) – second hidden layer.\n",
    "\n",
    "nn.Linear(32, 16) – third hidden layer.\n",
    "\n",
    "nn.Linear(16, 1) – output layer predicting a single continuous value.\n",
    "\n",
    "This kind of model can be used for tasks like:\n",
    "\n",
    "Predicting house prices based on 11 features (size, location, etc.)\n",
    "\n",
    "Estimating car mileage\n",
    "    \n",
    "Forecasting sales or demand\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa7b0e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nloss = F(y , y^)\\ny is a single integer ( class label )\\neg : y = 0 when y is a mammal and y^ is a tensor (prediction before the softmax )\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loss function tells us how good out model is during traning \n",
    "# Takes a model prediction y and ground truth y \n",
    "# Outputs a float \n",
    " # Class 0 - mamal , class 1 - bird , class 2 - reptile \n",
    "\n",
    "# Predicted class = 0 - > correct = low loss \n",
    "# Predicted class = 1 -> wrong = high loss \n",
    "# Predicted class = 2 - > wrong = high loss \n",
    "\n",
    "# Our goal is to minimise the loss \n",
    "\n",
    "''' \n",
    "loss = F(y , y^)\n",
    "y is a single integer ( class label )\n",
    "eg : y = 0 when y is a mammal and y^ is a tensor (prediction before the softmax )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dd47fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 0])\n",
      "tensor([0, 1, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'  \\nF.one_hot() converts class indices to one-hot vectors.\\n\\nCrossEntropyLoss expects raw logits and integer class labels, not one-hot.\\n\\nOne-hot targets are common in some contexts (like manual loss calculation), but not for CrossEntropyLoss.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "print(F.one_hot(torch.tensor(0), num_classes= 3 ))\n",
    "print(F.one_hot(torch.tensor(1), num_classes= 3 ))\n",
    "\n",
    "'''  \n",
    "F.one_hot() converts class indices to one-hot vectors.\n",
    "\n",
    "CrossEntropyLoss expects raw logits and integer class labels, not one-hot.\n",
    "\n",
    "One-hot targets are common in some contexts (like manual loss calculation), but not for CrossEntropyLoss.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b9a6267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.8222, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "scores = torch.tensor([-5.2, 4.6 , 0.8])\n",
    "one_hot_target = torch.tensor([1,0,0])\n",
    "criterion = CrossEntropyLoss()\n",
    "print(criterion(scores.double(), one_hot_target.double()))\n",
    "# scores - > model prediction before the final softmax function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a718b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot label: tensor([[0, 0, 1, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "y = [2]  # True class index\n",
    "scores = torch.tensor([[0.1, 6.0, -2.0, 3.2]])\n",
    "\n",
    "# Make sure the number of classes matches the scores\n",
    "num_classes = scores.size(1)\n",
    "\n",
    "# Create a one-hot encoded vector of the label y\n",
    "one_hot_label = F.one_hot(torch.tensor(y), num_classes=num_classes)\n",
    "\n",
    "print(\"One-hot label:\", one_hot_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6785d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted scores: tensor([[0.3353, 0.2086]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 0.7585461139678955\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.nn  import CrossEntropyLoss\n",
    "\n",
    "# Dummy input sample (e.g., batch size of 1, 16 features)\n",
    "sample = torch.randn(1, 16)\n",
    "\n",
    "# Target class index (e.g., class 1 out of 2 classes)\n",
    "target = torch.tensor([1])  # Not one-hot\n",
    "\n",
    "# Define the model\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 2)\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "prediction = model(sample)\n",
    "\n",
    "# Compute loss\n",
    "criterion = CrossEntropyLoss()\n",
    "loss = criterion(prediction, target)\n",
    "\n",
    "# Backward pass (compute gradients)\n",
    "loss.backward()\n",
    "\n",
    "# Output results\n",
    "print(\"Predicted scores:\", prediction)\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "\n",
    "lr = 0.001 \n",
    "#update weight\n",
    "weight = model[0].weight.grad \n",
    "\n",
    "weight = weight - lr * weight_grad \n",
    "\n",
    "\n",
    "bias = model[0].bias \n",
    "bias_grad = model[0].bias.grad \n",
    "bias = bias - lr * bias_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fcbec9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[-0.0016,  0.0710, -0.1597,  0.1314, -0.0631,  0.1496,  0.0433, -0.1203,\n",
      "          0.2097, -0.2100, -0.2428,  0.2067, -0.0999, -0.0224,  0.1810, -0.0837],\n",
      "        [ 0.0522,  0.0321, -0.0552,  0.0896,  0.0600, -0.0477,  0.1040, -0.1005,\n",
      "          0.1819,  0.0745, -0.0448,  0.0475,  0.0555,  0.0107,  0.1110,  0.0352],\n",
      "        [-0.1728,  0.2453, -0.1308,  0.0069,  0.1282, -0.0652, -0.1577,  0.1036,\n",
      "          0.0088, -0.1682,  0.1106, -0.2384,  0.2132, -0.0564, -0.0476,  0.0953],\n",
      "        [ 0.1532, -0.1204, -0.1741, -0.1092,  0.1336,  0.0855, -0.0914,  0.0178,\n",
      "         -0.0387,  0.1673, -0.0342,  0.0159, -0.1545, -0.1112, -0.1789,  0.1022],\n",
      "        [ 0.0872,  0.0486,  0.0877,  0.0915, -0.1102,  0.0166, -0.0491,  0.0310,\n",
      "         -0.0378, -0.0093,  0.0734,  0.0013, -0.1971,  0.1535,  0.1534,  0.0510],\n",
      "        [ 0.0990,  0.0209, -0.2135, -0.0952, -0.0942, -0.2168,  0.1342, -0.2235,\n",
      "          0.2083, -0.1552, -0.0822,  0.1147, -0.0501,  0.2043,  0.2265, -0.0890],\n",
      "        [ 0.1548, -0.1008,  0.1999,  0.2412, -0.0067,  0.1079,  0.0347, -0.1993,\n",
      "         -0.0082, -0.1225,  0.1362, -0.0584,  0.1574, -0.1645,  0.1429, -0.1081],\n",
      "        [-0.0250, -0.1784, -0.1514, -0.0412,  0.1114, -0.0868, -0.1547,  0.2369,\n",
      "          0.1662,  0.2108, -0.1464, -0.1160, -0.1752,  0.2071, -0.1635, -0.2022]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([0.1742, 0.1574], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "print(\"Weight of the first layer:\", weight_0)\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[1].bias\n",
    "print(\"Bias of the second layer:\", bias_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bc58624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming input_data is your input tensor (e.g., features from your dataset)\n",
    "# pred = model(input_data)\n",
    "\n",
    "# # Compute the loss\n",
    "# loss = criterion(pred, target)\n",
    "\n",
    "# # Perform backward pass to compute gradients\n",
    "# loss.backward()\n",
    "\n",
    "# # Update model's parameters using the optimizer\n",
    "# optimizer.step()\n",
    "\n",
    "# # Optionally, zero the gradients\n",
    "# optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1054902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight of the first layer: Parameter containing:\n",
      "tensor([[-0.0545,  0.2350,  0.1334, -0.2137, -0.1480,  0.0091, -0.0921,  0.1934,\n",
      "         -0.0367, -0.0410, -0.2262,  0.1300,  0.0238,  0.0848, -0.0232, -0.2380],\n",
      "        [-0.0800,  0.0611,  0.2315,  0.2148, -0.1206,  0.1076, -0.1219, -0.1809,\n",
      "         -0.0708, -0.0717,  0.0088, -0.0642, -0.1042, -0.1648, -0.0629, -0.0540],\n",
      "        [ 0.1518, -0.1828, -0.2389,  0.0515,  0.0201, -0.2300, -0.2214, -0.2029,\n",
      "          0.1064, -0.1619, -0.1913,  0.0437,  0.0385, -0.0733,  0.1596,  0.0068],\n",
      "        [-0.0049,  0.1370,  0.0761, -0.0721,  0.1083,  0.1089, -0.1589, -0.0835,\n",
      "          0.0488,  0.0819, -0.0580,  0.0243,  0.1734, -0.1749,  0.2279, -0.0200],\n",
      "        [ 0.0183,  0.1573,  0.1035,  0.0418, -0.1157,  0.0686, -0.2041, -0.2423,\n",
      "          0.0886, -0.2293, -0.1751, -0.0344,  0.1027,  0.0513,  0.1926, -0.1265],\n",
      "        [ 0.1844, -0.1980, -0.1908, -0.0189, -0.1149,  0.2409,  0.0301, -0.0925,\n",
      "          0.0381, -0.0279, -0.2285,  0.0685, -0.0602,  0.1715,  0.1536,  0.1152],\n",
      "        [ 0.2474,  0.0752,  0.2398,  0.1150, -0.2377, -0.0722, -0.2248,  0.0115,\n",
      "          0.2392,  0.0534,  0.1347,  0.1358, -0.0703, -0.1459,  0.1654,  0.2289],\n",
      "        [ 0.1709, -0.1684, -0.2023, -0.2322, -0.0554,  0.0728,  0.1562, -0.0468,\n",
      "         -0.1324,  0.0127,  0.1828,  0.0558, -0.2404,  0.2255, -0.1952,  0.0744]],\n",
      "       requires_grad=True)\n",
      "Bias of the second layer: Parameter containing:\n",
      "tensor([0.2841, 0.2297], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Create a model with ReLU activation function between layers\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(16, 8),\n",
    "    nn.ReLU(),  # Adding ReLU activation function\n",
    "    nn.Linear(8, 2)\n",
    ")\n",
    "\n",
    "# Access the weight of the first linear layer\n",
    "weight_0 = model[0].weight\n",
    "print(\"Weight of the first layer:\", weight_0)\n",
    "\n",
    "# Access the bias of the second linear layer\n",
    "bias_1 = model[2].bias  # We are accessing the second linear layer after ReLU\n",
    "print(\"Bias of the second layer:\", bias_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "570fa3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.5076\n"
     ]
    }
   ],
   "source": [
    "# Regression model in relu function\n",
    "''' Keras Regression Model with ReLU'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the model\n",
    "class SimpleRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleRegressionModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 1)  # Single output unit (for regression)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Example input and target\n",
    "inputs = torch.randn(5, 10)  # 5 samples, 10 features each\n",
    "targets = torch.randn(5, 1)  # 5 target values\n",
    "\n",
    "# Create the model\n",
    "model = SimpleRegressionModel(input_size=10)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop (one iteration for simplicity)\n",
    "optimizer.zero_grad()     # Zero the gradients\n",
    "outputs = model(inputs)   # Forward pass\n",
    "loss = criterion(outputs, targets)  # Calculate loss\n",
    "loss.backward()           # Backward pass\n",
    "optimizer.step()          # Update the weights\n",
    "\n",
    "print(f'Loss: {loss.item():.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789fe6c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchmetrics'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchmetrics\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# --- Layer Initialization ---\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchmetrics'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "# --- Layer Initialization ---\n",
    "layer0 = nn.Linear(16, 32)\n",
    "layer1 = nn.Linear(32, 64)\n",
    "\n",
    "# Initialize weights with uniform distribution\n",
    "nn.init.uniform_(layer0.weight)\n",
    "nn.init.uniform_(layer1.weight)\n",
    "\n",
    "model_layers = nn.Sequential(layer0, layer1)\n",
    "\n",
    "# --- Evaluation Loop ---\n",
    "def evaluate(model, validationloader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    validation_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in validationloader:\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    validation_loss_epoch = validation_loss / len(validationloader)\n",
    "    print(f\"Validation loss: {validation_loss_epoch:.4f}\")\n",
    "\n",
    "    model.train()  # Set back to training mode\n",
    "    return validation_loss_epoch\n",
    "\n",
    "# --- Accuracy calculation using torchmetrics ---\n",
    "def accuracy_epoch(model, dataloader):\n",
    "    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=3)\n",
    "    for features, labels in dataloader:\n",
    "        outputs = model(features)\n",
    "        metric.update(outputs, labels.argmax(dim=-1))\n",
    "\n",
    "    accuracy = metric.compute()\n",
    "    print(f\"Accuracy on all data: {accuracy:.4f}\")\n",
    "    metric.reset()\n",
    "    return accuracy\n",
    "\n",
    "# --- Dropout example ---\n",
    "def dropout_example(features):\n",
    "    model = nn.Sequential(\n",
    "        nn.Linear(8, 6),\n",
    "        nn.Linear(6, 4),\n",
    "        nn.Dropout(p=0.5)\n",
    "    )\n",
    "    # Train mode (Dropout active)\n",
    "    model.train()\n",
    "    output_train = model(features)\n",
    "\n",
    "    # Eval mode (Dropout disabled)\n",
    "    model.eval()\n",
    "    output_eval = model(features)\n",
    "\n",
    "    print(\"Output in train mode:\", output_train)\n",
    "    print(\"Output in eval mode:\", output_eval)\n",
    "\n",
    "# --- Random Search for Hyperparameters ---\n",
    "def random_search():\n",
    "    values = []\n",
    "    for idx in range(10):\n",
    "        # Randomly sample a learning rate factor between 2 and 4\n",
    "        factor = np.random.uniform(2, 4)\n",
    "        lr = 10 ** -factor\n",
    "        \n",
    "        # Randomly select momentum between 0.85 and 0.99\n",
    "        momentum = np.random.uniform(0.85, 0.99)\n",
    "        \n",
    "        values.append((lr, momentum))\n",
    "    # Assuming plot_hyperparameter_search(values) is defined elsewhere\n",
    "    # plot_hyperparameter_search(values)\n",
    "    return values\n",
    "\n",
    "# --- Example usage (assuming DataLoader, criterion, etc. are defined) ---\n",
    "# features, labels, validationloader, dataloader, and criterion should be defined in your actual usage context\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example tensor for dropout example (batch_size=5, features=8)\n",
    "    example_features = torch.randn(5, 8)\n",
    "    dropout_example(example_features)\n",
    "\n",
    "    # Run random search and print results\n",
    "    hyperparams = random_search()\n",
    "    print(\"Random search results (lr, momentum):\")\n",
    "    for lr, mom in hyperparams:\n",
    "        print(f\"LR: {lr:.6f}, Momentum: {mom:.4f}\")\n",
    "\n",
    "    # Evaluation and accuracy require actual data loaders and criterion\n",
    "    # Uncomment and provide those to run:\n",
    "    # val_loss = evaluate(model_layers, validationloader, criterion)\n",
    "    # acc = accuracy_epoch(model_layers, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d5ce79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
